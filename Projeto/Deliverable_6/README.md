# Deliverable 5

<p align="center">
    <img src="https://github.com/Rafael-Kauati/CSLP-projects/blob/main/Projeto/Deliverable_5/ScreenShots/tests_0.png?raw=true" height="450px">
    <img src="https://github.com/Rafael-Kauati/CSLP-projects/blob/main/Projeto/Deliverable_5/ScreenShots/tests_1.png?raw=true" height="450px">
    <img src="https://github.com/Rafael-Kauati/CSLP-projects/blob/main/Projeto/Deliverable_5/ScreenShots/tests_2.png?raw=true" height="450px">
</p>

## Description

This deliverable encompasses implementing lossless hybrid video codec implemented using part of the techniques and classes created for previous deliverables. 

This codec relies on the same algorithm as deliverable 4's (JPEG 6) but now it features inter-frame coding to complement the intra-frame coding made in the last deliverable.

The block size, search area, step size and intra-frame frequency are given by the user but we tested the code multiple times to obtain the best default options we could. 


## IMPORTANT PREDICTOR UPDATE

Just after concluding the deliverable 5, we noticed a glaring error in our hybrid codec that would increase our fina .bin file to over five times the intended size.

Our internal difference blocks (of the previous frame's best block agains our current frame's current block) were created inside a openCV mat block, which is a unsigned character that can store values between 0 and 255.

When our difference was calculated and stored inside the Mat, if the difference was negative our actualy stored value would underflow to 255, and since we are using a 2 x |n| encoded value to account for negative values, the final bit encoded value for a -1 integer would be about 510!

This causes some time performance loss (since more bit would be encoded), potential memory limit overflow for large frames/block sizes and, most of all, enormous binary block sizes.

The issue as since been fixed by changing the Mat variables to signed integer 2D vectors, which reduced our .bin file sizes to about one fifth of last's deliverables size for the same 4:4:4 YUV files.

## File System

The tests are inside the root (of the deliverable).

- Input image/video files are stored inside the TestFiles/ folder.
- Binary files are stored inside the BinOutput/ folder.
- Output images/videos are stored inside the OutputFiles/ folder.
- Required classes are stored inside the Predictor/ and Golomb/ folders.

```
. # Deliverable 4
├── CMakeFiles
│
├── Golomb
│   ├── Golomb.cpp
│   ├── Golomb_Decoder.cpp
│   ├── Golomb_Decoder.hpp
│   ├── Golomb_Encoder.cpp
│   └── Golomb_Encoder.hpp
│
├── OutputFiles
│   └── output.mp4
│
├── HybridCodec
│   ├── BitStream.h
│   ├── BlockSearch.cpp
│   ├── Frame_Predicter.h
│   └── HybridCodec.cpp
│
├── ScreenShots
│   ├── tests_0.png
│   ├── tests_1.png
│   └── tests_2.png
│
├── TestFiles
│   ├── ducks_30_frames.y4m
│   ├── ducks_7_frames.y4m
│   ├── testVideo.mp4
│   └── test.png
│
├── BinOutput
│   └── output.bin
│
├── CMakeLists.txt
├── auto-cmake.sh
├── clean.sh
├── README.md
└── unit_tests.cpp
```

## Code Components
#### Frame Predicter, Golomb and BitStream
All these classes where imported from the previous deliverables, and the colomb classes where modified to work better with blocks of frames (blocks of numbers to encode/decode).

#### HybridCodec
This class is responsible of managing the Frame Predicter (for intra-frame coding) aswell as calculating and interacting with the Colomb classes to encode/decode the blocks and positions needed to implement the inter-frame coding.


The HybridCodec's inter-frame coding takes into account the defined variables given by the user (block size, frequency, etc) and codes the frames based on the selected coding for each.

The inter-frame encodings are based on the previous frame's values, where the most similar found block inside the last frame is subtracted from the current frame and the coordinate difference between both is also encoded.

When decoding, the current block is generated by simple getting the block of the last frame (current coordinates + the offset decoded) and adding them to the difference block.

#### Unit Tests
The unit tests for this deliverable is simple.

Encode a video and decode it, then do a complete check of the integrity of the final decoded file against all the pixels of the original file.

This unit test can also be used as a "template" for using our final video codec on other projects.


## How to test
Similar to the other deliverables, our unit tests can be run by:

1- Compiling the required files
```bash
# assuming you're in the program's dir
cmake . && make
```
2- Executing the unit tests
```bash
./unit_tests
```


## OPENCV Disclosure
In this code and the previous deliverable 4, we use openCV to manage videos and frames.

The OpenCV library converts YUV frames to RGB frames internaly, and while the option for maintaning YUV compatibility was once implemented, it was removed.

We must keep in mind that a lot of the large .bin files created by our supposed "compression" algorithm became twice as large as the original file largely due to this fact, and with some more time parts of our code should be rewritten to remove the need for OpenCV, so as to manipulate the input YUV videos manually.


## Variable Tests
First, we defined all the variables as the smallest considered value:
```
Search_Size = 2;
Block_Size = 2;
Step_Size = 2;
```
---

Then we started by choosing a search size:

    - Search Size = 2: 
        -> time: 40 694 / 42 194
        -> size of bin: 254,9 MiB (267 244 974 Bytes)

    - Search Size = 4:
        -> time: 64 875 / 45 024
        -> size of bin: 252,1 MiB (264 390 667 Bytes)

    - Search Size = 8:
        -> time: 179 876 / 38 400
        -> size of bin: 249,8 MiB (261 947 552 Bytes)

With these results we decided the best search size for our search size will be 4, since it doesn't affect the execution times as much and gives a good balance of performance / final binary file size.

To be noted that this value only affects the encode time, since the value changes the complexity of finding the best possible corresponding block on the previous frame, if more compression is needed we could increase the value.

---

After the last test, we set our search size to 4 and tested the block size: 

    - Block Size = 2: 
        -> time: 77 850 / 44 704
        -> size of bin: 252,1 MiB (264 390 667 Bytes)

    - Block Size = 4:
        -> time: 59 619 / 32 843
        -> size of bin: 246,7 MiB (258 693 421 Bytes)

    - Block Size = 8:
        -> time: 51 002 / 29 750
        -> size of bin: 245,9 MiB (257 823 255 Bytes)

    - Block Size = 16:
        -> time: 45 821 / 34 425
        -> size of bin: 246,0 MiB (257 922 738 Bytes)

Analysing these results we selected a block size of 8, since it provides the best final file size and reduces performance impact, since bigger blocks cause less total blocks, which in turn reduces the times we need to call the function to get the most simillar block of the previous frame, which is our most computational demanding function.

---

After the last test, we set our block size to 8 and tested the step size: 

    - Step Size = 2: 
        -> time: 53 686 / 32 714
        -> size of bin: 245,9 MiB (257 823 255 Bytes)

    - Step Size = 4:
        -> time: 31 691 / 33 206
        -> size of bin: 246,9 MiB (258 883 399 Bytes)

    - Step Size = 8:
        -> time: 22 625 / 34 603
        -> size of bin: 251,7 MiB (263 929 847 Bytes)

    - Step Size = 16:
        -> time: 20 866 / 29 928
        -> size of bin: 251,7 MiB (263 929 847 Bytes)

With these values, we chose a step size of 4 since it gives a good compression ration without causing big performance losses like a smaller step size would.


## Parameters chosen
After the running previous tests, we decided these were the best parameters for such an implementation as ours:
```
Search_Size = 4;
Block_Size = 8;
Step_Size = 4;
```

It must be noted that inter-frame frequency is also a parameter to keep in mind, but since it mostly impacts reliability, we chose to keep it at a value between 3 and 6.

## Conclusions
Concluding, we found that these parameters greatly impact performance and compression ratios, and must be carefully choosen for any given final implementation.

This algorithm became much more complex and resource consuming than antecipated, but we believe with some more code optimizations a better performance can be obtain.

Although we must keep in mind that a lot of these functions are inherently resource-heavy, and no amount of optimization can bring them a lot closer to perfect.
